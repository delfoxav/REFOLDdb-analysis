{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40c5ca67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import textwrap\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "261da170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1875 entries, 0 to 1874\n",
      "Data columns (total 28 columns):\n",
      " #   Column                      Non-Null Count  Dtype  \n",
      "---  ------                      --------------  -----  \n",
      " 0   pubmed_id                   1841 non-null   float64\n",
      " 1   title                       1841 non-null   object \n",
      " 2   abstract                    1839 non-null   object \n",
      " 3   date                        1841 non-null   object \n",
      " 4   author                      1841 non-null   object \n",
      " 5   journal                     1841 non-null   object \n",
      " 6   name                        1875 non-null   object \n",
      " 7   aaseq                       1852 non-null   object \n",
      " 8   comment                     1175 non-null   object \n",
      " 9   uniprot_id                  1788 non-null   object \n",
      " 10  function                    640 non-null    object \n",
      " 11  domain                      532 non-null    object \n",
      " 12  ph                          1804 non-null   float64\n",
      " 13  temperature                 1708 non-null   object \n",
      " 14  dilution                    1875 non-null   object \n",
      " 15  dialysis                    1875 non-null   object \n",
      " 16  column_filtration           1875 non-null   object \n",
      " 17  column_binding              1875 non-null   object \n",
      " 18  high_pressure               1875 non-null   object \n",
      " 19  other_method                1875 non-null   object \n",
      " 20  activity                    1875 non-null   object \n",
      " 21  solubility                  1875 non-null   object \n",
      " 22  non_aggregability           1875 non-null   object \n",
      " 23  circular_dichroism          1875 non-null   object \n",
      " 24  fluorescence_tryptophan     1875 non-null   object \n",
      " 25  nuclear_magnetic_resonance  1875 non-null   object \n",
      " 26  crystallization             1875 non-null   object \n",
      " 27  structure_determination     1875 non-null   object \n",
      "dtypes: float64(2), object(26)\n",
      "memory usage: 410.3+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Load the downloaded DB as a pandas DF\n",
    "\n",
    "df = pd.read_table(\"REFOLD_161028.txt\")\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a1e41293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pubmed_id                                                                   NaN\n",
      "title                                                                       NaN\n",
      "abstract                                                                    NaN\n",
      "date                                                                        NaN\n",
      "author                                                                      NaN\n",
      "journal                                                                     NaN\n",
      "name                                   Major histocompatibility complex class I\n",
      "aaseq                         MEPSLLSLFVLGVVALTETRAGSHSLRYFDTAMSRPELGDSQFISV...\n",
      "comment                                                                     NaN\n",
      "uniprot_id                                                               Q9TPK7\n",
      "function                                                                    NaN\n",
      "domain                                                                      NaN\n",
      "ph                                                                          8.0\n",
      "temperature                                                                   4\n",
      "dilution                                                                      f\n",
      "dialysis                                                                      t\n",
      "column_filtration                                                             f\n",
      "column_binding                                                                f\n",
      "high_pressure                                                                 f\n",
      "other_method                                                                  f\n",
      "activity                                                                      f\n",
      "solubility                                                                    f\n",
      "non_aggregability                                                             f\n",
      "circular_dichroism                                                            f\n",
      "fluorescence_tryptophan                                                       f\n",
      "nuclear_magnetic_resonance                                                    f\n",
      "crystallization                                                               f\n",
      "structure_determination                                                       f\n",
      "Name: 137, dtype: object\n"
     ]
    }
   ],
   "source": [
    "for index, row in df.iterrows():\n",
    "    if pd.isna(row[\"date\"]):\n",
    "        print(row)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d165ac59",
   "metadata": {},
   "source": [
    "It looks like the newer DB is missing some important fields such as the solvant used in the experiments. For the older records (before 2009), the information could be scrapped from https://pford.info/refolddatabase/. But for the newer records, it might be necessary to explore the papers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d82684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract a single record and explore it to see how it looks like\n",
    "\n",
    "record = df.sample(1).iloc[0]\n",
    "\n",
    "for col, val in record.items():\n",
    "    val_str = str(val)\n",
    "    wrapped = textwrap.fill(val_str, width=80)  # wrap at 80 chars\n",
    "    print(f\"**{col}**: {wrapped}\")\n",
    "    print(\"*\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2415ada",
   "metadata": {},
   "source": [
    "As the records don't even contains the link to to Monash University REFOLD database, we will have to match them ourselves. First we scrap Monash University REFOLD database to get the list of records.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b25ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we gather the columns name for our DB using a single record\n",
    "r = requests.get(\"https://pford.info/refolddatabase/refoldingrecord/5/\")\n",
    "\n",
    "soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "header = \"\"\n",
    "col_names = []\n",
    "data = []\n",
    "\n",
    "for th in soup.find_all(\"th\"):\n",
    "    if th.get(\"class\")[0] == \"detail_header\":\n",
    "        header = th.find(string=True, recursive=False).strip()\n",
    "    else:\n",
    "        col_names.append(header + \".\" + th.text.replace(\" \",\"_\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5251aa0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gatherrecord(url):\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "\n",
    "    data = []\n",
    "    for row in soup.find_all('tr'):\n",
    "            for cell in row.find_all(\"td\"):\n",
    "                data.append(cell.text)\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def analyze_page(url):\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "    page_data = []\n",
    "    for row in tqdm(soup.find_all('tr'), unit=\"record\"):\n",
    "        link = row.find(\"a\")\n",
    "        if link: # Deal with the headers \n",
    "            url = \"https://pford.info/refolddatabase/refoldingrecord/\" + link.get(\"href\")\n",
    "            row_data = gatherrecord(url)\n",
    "            page_data.append(row_data)\n",
    "                \n",
    "\n",
    "    return page_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f596e4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next we explore the full website\n",
    "\n",
    "full_data = []\n",
    "\n",
    "# We start from the start page and do it manually (as we need to have a next page)\n",
    "start_time = time.time()\n",
    "url = \"https://pford.info/refolddatabase/refoldingrecord\"\n",
    "r = requests.get(url)\n",
    "soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "page_data = analyze_page(url)\n",
    "    \n",
    "full_data.extend(page_data)\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(\"It took \"+ str(elapsed_time) + \" to deal with the page\")\n",
    "\n",
    "\n",
    "# We loop while we have a next page\n",
    "while soup.find(\"a\", string=\"Next\"):\n",
    "    start_time = time.time()\n",
    "\n",
    "    url = \"https://pford.info/refolddatabase/refoldingrecord\"+soup.find(\"a\", string=\"Next\").get(\"href\")\n",
    "    print(\"Dealing with : \"+soup.find(\"a\", string=\"Next\").get(\"href\") + \"/47\")\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "    page_data = analyze_page(url)\n",
    "    full_data.extend(page_data)\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(\"It took \"+ str(elapsed_time) + \" to deal with the page\")\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c7197f",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b01ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " \n",
    "\n",
    "full_df = pd.DataFrame(full_data, columns=col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a043d756",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "url = \"https://pford.info/refolddatabase/refoldingrecord\"\n",
    "r = requests.get(url)\n",
    "soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "data_columns = [th.text.strip() for th in soup.find_all('th')]\n",
    "data_columns.append(\"url\")\n",
    "data = []\n",
    "for row in soup.find_all('tr'):\n",
    "    row_data = []\n",
    "    for cell in row.find_all(\"td\"):\n",
    "        link = cell.find(\"a\")\n",
    "        if link:\n",
    "            url = \"https://pford.info/refolddatabase/refoldingrecord/\" + link.get(\"href\")\n",
    "        row_data.append(cell.text)\n",
    "    row_data.append(url)\n",
    "    data.append(row_data)\n",
    "\n",
    "# remove the first entry as it is the headers\n",
    "data = data[1:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf170948",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "while soup.find(\"a\", string=\"Next\"):\n",
    "    url = \"https://pford.info/refolddatabase/refoldingrecord\"+soup.find(\"a\", string=\"Next\").get(\"href\")\n",
    "    r = requests.get(url)\n",
    "    sub_data = []\n",
    "    soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "    for row in soup.find_all('tr'):\n",
    "        row_data = []\n",
    "        for cell in row.find_all(\"td\"):\n",
    "            link = cell.find(\"a\")\n",
    "            if link:\n",
    "                url = \"https://pford.info/refolddatabase/refoldingrecord/\" + link.get(\"href\")\n",
    "            row_data.append(cell.text)\n",
    "        row_data.append(url)\n",
    "        sub_data.append(row_data)\n",
    "    \n",
    "\n",
    "    # remove the first entry as it is the headers\n",
    "    sub_data = sub_data[1:]\n",
    "\n",
    "    data.extend(sub_data)\n",
    "    break\n",
    "\n",
    "Monash_df = pd.DataFrame(data, columns=data_columns)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388adc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all(\"th\")[0].find(string=True, recursive=False).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39174e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(\"https://pford.info/refolddatabase/refoldingrecord/5/\")\n",
    "\n",
    "soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "header = \"\"\n",
    "col_names = []\n",
    "data = []\n",
    "\n",
    "for th in soup.find_all(\"th\"):\n",
    "    if th.get(\"class\")[0] == \"detail_header\":\n",
    "        header = th.find(string=True, recursive=False).strip()\n",
    "    else:\n",
    "        col_names.append(header + \".\" + th.text.replace(\" \",\"_\"))\n",
    "\n",
    "\n",
    "for row in soup.find_all('tr'):\n",
    "        for cell in row.find_all(\"td\"):\n",
    "            data.append(cell.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9841a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3cc3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# counts per uniprot_id (including NaN)\n",
    "counts = df['uniprot_id'].value_counts(dropna=False)\n",
    "print(counts.head(20))\n",
    "\n",
    "# summary stats\n",
    "total = len(df)\n",
    "unique = df['uniprot_id'].nunique(dropna=True)\n",
    "n_missing = df['uniprot_id'].isna().sum()\n",
    "n_duplicated_ids = (counts > 1).sum()\n",
    "\n",
    "print(f\"total rows: {total}\")\n",
    "print(f\"unique uniprot_id (non-null): {unique}\")\n",
    "print(f\"missing uniprot_id: {n_missing}\")\n",
    "print(f\"uniprot_ids appearing more than once: {n_duplicated_ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072283ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549100ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
