{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c5ca67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import textwrap\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261da170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the downloaded DB as a pandas DF\n",
    "\n",
    "df = pd.read_table(\"REFOLD_161028.txt\")\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d165ac59",
   "metadata": {},
   "source": [
    "It looks like the newer DB is missing some important fields such as the solvant used in the experiments. For the older records (before 2009), the information could be scrapped from https://pford.info/refolddatabase/. But for the newer records, it might be necessary to explore the papers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d82684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract a single record and explore it to see how it looks like\n",
    "\n",
    "record = df.sample(1).iloc[0]\n",
    "\n",
    "for col, val in record.items():\n",
    "    val_str = str(val)\n",
    "    wrapped = textwrap.fill(val_str, width=80)  # wrap at 80 chars\n",
    "    print(f\"**{col}**: {wrapped}\")\n",
    "    print(\"*\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2415ada",
   "metadata": {},
   "source": [
    "As the records don't even contains the link to to Monash University REFOLD database, we will have to match them ourselves. First we scrap Monash University REFOLD database to get the list of records.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b25ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we gather the columns name for our DB using a single record\n",
    "r = requests.get(\"https://pford.info/refolddatabase/refoldingrecord/5/\")\n",
    "\n",
    "soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "header = \"\"\n",
    "col_names = []\n",
    "data = []\n",
    "\n",
    "for th in soup.find_all(\"th\"):\n",
    "    if th.get(\"class\")[0] == \"detail_header\":\n",
    "        header = th.find(string=True, recursive=False).strip()\n",
    "    else:\n",
    "        col_names.append(header + \".\" + th.text.replace(\" \",\"_\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5251aa0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gatherrecord(url):\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "\n",
    "    data = []\n",
    "    for row in soup.find_all('tr'):\n",
    "            for cell in row.find_all(\"td\"):\n",
    "                data.append(cell.text)\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def analyze_page(url):\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "    page_data = []\n",
    "    for row in tqdm(soup.find_all('tr'), unit=\"record\"):\n",
    "        link = row.find(\"a\")\n",
    "        if link: # Deal with the headers \n",
    "            url = \"https://pford.info/refolddatabase/refoldingrecord/\" + link.get(\"href\")\n",
    "            row_data = gatherrecord(url)\n",
    "            page_data.append(row_data)\n",
    "                \n",
    "\n",
    "    return page_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f596e4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next we explore the full website\n",
    "\n",
    "full_data = []\n",
    "\n",
    "# We start from the start page and do it manually (as we need to have a next page)\n",
    "start_time = time.time()\n",
    "url = \"https://pford.info/refolddatabase/refoldingrecord\"\n",
    "r = requests.get(url)\n",
    "soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "page_data = analyze_page(url)\n",
    "    \n",
    "full_data.extend(page_data)\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(\"It took \"+ str(elapsed_time) + \" to deal with the page\")\n",
    "\n",
    "\n",
    "# We loop while we have a next page\n",
    "while soup.find(\"a\", string=\"Next\"):\n",
    "    start_time = time.time()\n",
    "\n",
    "    url = \"https://pford.info/refolddatabase/refoldingrecord\"+soup.find(\"a\", string=\"Next\").get(\"href\")\n",
    "    print(\"Dealing with : \"+soup.find(\"a\", string=\"Next\").get(\"href\") + \"/47\")\n",
    "    r = requests.get(url)\n",
    "    soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "    page_data = analyze_page(url)\n",
    "    full_data.extend(page_data)\n",
    "\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(\"It took \"+ str(elapsed_time) + \" to deal with the page\")\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c7197f",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b01ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " \n",
    "\n",
    "full_df = pd.DataFrame(full_data, columns=col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a043d756",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "url = \"https://pford.info/refolddatabase/refoldingrecord\"\n",
    "r = requests.get(url)\n",
    "soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "data_columns = [th.text.strip() for th in soup.find_all('th')]\n",
    "data_columns.append(\"url\")\n",
    "data = []\n",
    "for row in soup.find_all('tr'):\n",
    "    row_data = []\n",
    "    for cell in row.find_all(\"td\"):\n",
    "        link = cell.find(\"a\")\n",
    "        if link:\n",
    "            url = \"https://pford.info/refolddatabase/refoldingrecord/\" + link.get(\"href\")\n",
    "        row_data.append(cell.text)\n",
    "    row_data.append(url)\n",
    "    data.append(row_data)\n",
    "\n",
    "# remove the first entry as it is the headers\n",
    "data = data[1:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf170948",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "while soup.find(\"a\", string=\"Next\"):\n",
    "    url = \"https://pford.info/refolddatabase/refoldingrecord\"+soup.find(\"a\", string=\"Next\").get(\"href\")\n",
    "    r = requests.get(url)\n",
    "    sub_data = []\n",
    "    soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "    for row in soup.find_all('tr'):\n",
    "        row_data = []\n",
    "        for cell in row.find_all(\"td\"):\n",
    "            link = cell.find(\"a\")\n",
    "            if link:\n",
    "                url = \"https://pford.info/refolddatabase/refoldingrecord/\" + link.get(\"href\")\n",
    "            row_data.append(cell.text)\n",
    "        row_data.append(url)\n",
    "        sub_data.append(row_data)\n",
    "    \n",
    "\n",
    "    # remove the first entry as it is the headers\n",
    "    sub_data = sub_data[1:]\n",
    "\n",
    "    data.extend(sub_data)\n",
    "    break\n",
    "\n",
    "Monash_df = pd.DataFrame(data, columns=data_columns)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388adc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find_all(\"th\")[0].find(string=True, recursive=False).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39174e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(\"https://pford.info/refolddatabase/refoldingrecord/5/\")\n",
    "\n",
    "soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "header = \"\"\n",
    "col_names = []\n",
    "data = []\n",
    "\n",
    "for th in soup.find_all(\"th\"):\n",
    "    if th.get(\"class\")[0] == \"detail_header\":\n",
    "        header = th.find(string=True, recursive=False).strip()\n",
    "    else:\n",
    "        col_names.append(header + \".\" + th.text.replace(\" \",\"_\"))\n",
    "\n",
    "\n",
    "for row in soup.find_all('tr'):\n",
    "        for cell in row.find_all(\"td\"):\n",
    "            data.append(cell.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9841a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(col_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3cc3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# counts per uniprot_id (including NaN)\n",
    "counts = df['uniprot_id'].value_counts(dropna=False)\n",
    "print(counts.head(20))\n",
    "\n",
    "# summary stats\n",
    "total = len(df)\n",
    "unique = df['uniprot_id'].nunique(dropna=True)\n",
    "n_missing = df['uniprot_id'].isna().sum()\n",
    "n_duplicated_ids = (counts > 1).sum()\n",
    "\n",
    "print(f\"total rows: {total}\")\n",
    "print(f\"unique uniprot_id (non-null): {unique}\")\n",
    "print(f\"missing uniprot_id: {n_missing}\")\n",
    "print(f\"uniprot_ids appearing more than once: {n_duplicated_ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072283ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549100ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
